{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read recipe final text file, index_col=[0] is used to eliminate extra index column\n",
    "df_words = pd.read_csv(\"recipe_final.txt\",index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw recipe file and leave only name and id column\n",
    "df_raw = pd.read_csv(\"Resources/RAW_recipes.csv\")\n",
    "df_raw = df_raw[['name','id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter recipes that only have more than 300 reviews\n",
    "df_words = df_words.sort_values('review', ascending = False)\n",
    "df_words_filtered = df_words.loc[df_words['review'] > 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2 dataframes\n",
    "df_words_filtered = pd.merge(df_words_filtered,df_raw, how=\"inner\",on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave \n",
    "scrape_final = df_words_filtered[['id','name']]\n",
    "scrape_final[\"id\"] = scrape_final[\"id\"].apply(str)\n",
    "scrape_final[\"recipe_url\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each recipe id, create a recipe url to query and save in dataframe\n",
    "for x in range(len(scrape_final)):\n",
    "    recipe_id = scrape_final['id'][x]\n",
    "    website_url = f\"https://www.food.com/recipe-{recipe_id}\"\n",
    "    scrape_final[\"recipe_url\"][x] = website_url\n",
    "    scrape_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list = [x.replace('  ', '') for x in url_list]\n",
    "# url_list = [x.replace(' ', '-') for x in url_list]\n",
    "# len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up executable path\n",
    "executable_path = {\"executable_path\": \"chromedriver\"}\n",
    "browser = Browser(\"chrome\", **executable_path, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the picture of the first recipe and save\n",
    "json_output = []\n",
    "for index,item in enumerate(scrape_final[\"recipe_url\"]):\n",
    "    try:\n",
    "        print(index, item)\n",
    "        time.sleep(7)\n",
    "        browser.visit(item)\n",
    "        time.sleep(7)\n",
    "        website_html = browser.html\n",
    "        website_soup = BeautifulSoup(website_html, \"html.parser\")\n",
    "        website_results = website_soup.find(\"div\", class_=\"inner-wrapper\")\n",
    "        print(website_results)\n",
    "        picture_url = website_results.img[\"data-src\"]\n",
    "        recipe_id = scrape_final[\"id\"][index]\n",
    "        json_output.append({recipe_id: {\"name\": scrape_final[\"name\"][index],\"recipe_img\": picture_url}})\n",
    "        \n",
    "    except:\n",
    "        print(\"*** Review URL ***\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output as json file\n",
    "import json\n",
    "with open('recipe_url.json', 'w') as f:\n",
    "    json.dump(json_output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scrape the picture of the first recipe and save\n",
    "# for recipe in url_list:\n",
    "#     for item in range(len(scrape_final)):\n",
    "#         try:\n",
    "#             print(recipe)\n",
    "#             time.sleep(7)\n",
    "#             browser.visit(recipe)\n",
    "#             time.sleep(7)\n",
    "#             website_html = browser.html\n",
    "#             website_soup = BeautifulSoup(website_html, \"html.parser\")\n",
    "#             website_results = website_soup.find(\"div\", class_=\"inner-wrapper\")\n",
    "#             print(website_results)\n",
    "#             picture_url = website_results.img[\"data-src\"]\n",
    "# #             picture_list.append(picture_url)\n",
    "#             json_output.append({scrape_final[\"id\"][item]: {\"name\": scrape_final[\"name\"][item]},\"recipe_img\": picture_url})\n",
    "#         except:\n",
    "#             print(\"*** Review URL ***\")\n",
    "#             print(recipe)\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipeid, name, recipe_url, recipe image url\n",
    "\n",
    "# json = [{recipeid: {name: brocolli,\n",
    "#              image: image_url},\n",
    "#  {recipeid: {name: brocolli,\n",
    "#              image: image_url},]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('PythonData': conda)",
   "language": "python",
   "name": "python36964bitpythondataconda01d547e64dad45c8ade711e333fb7faa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
